% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mr_mash.R
\name{mr.mash}
\alias{mr.mash}
\title{Multiple Regression with Multivariate Adaptive Shrinkage.}
\usage{
mr.mash(
  X,
  Y,
  S0,
  w0 = rep(1/(length(S0)), length(S0)),
  V = cov(Y),
  mu1_init = matrix(0, nrow = ncol(X), ncol = ncol(Y)),
  tol = 1e-04,
  max_iter = 5000,
  update_w0 = TRUE,
  update_w0_method = c("EM", "mixsqp"),
  compute_ELBO = TRUE,
  standardize = TRUE,
  verbose = TRUE,
  update_V = FALSE,
  version = c("Rcpp", "R"),
  e = 1e-08,
  ca_update_order = c("consecutive", "decreasing_logBF", "increasing_logBF")
)
}
\arguments{
\item{X}{n x p matrix of covariates.}

\item{Y}{n x r matrix of responses.}

\item{S0}{List of length K containing the desired r x r prior
covariance matrices on the regression coefficients.}

\item{w0}{K-vector with prior mixture weights, each associated with
the respective covariance matrix in \code{S0}.}

\item{V}{r x r residual covariance matrix.}

\item{mu1_init}{p x r matrix of initial estimates of the posterior
mean regression coefficients.}

\item{tol}{Convergence tolerance.}

\item{max_iter}{Maximum number of iterations for the optimization
algorithm.}

\item{update_w0}{If \code{TRUE}, prior weights are updated.}

\item{update_w0_method}{Method to update prior weights.}

\item{compute_ELBO}{If \code{TRUE}, ELBO is computed.}

\item{standardize}{If \code{TRUE}, X is "standardized" using the
sample means and sample standard deviations. Standardizing X
allows a faster implementation, but the prior has a different
interpretation. Coefficients and covariances are returned on the
original scale.}

\item{verbose}{If \code{TRUE}, some information about the
algorithm's process is printed at each iteration.}

\item{update_V}{if \code{TRUE}, residual covariance is updated.}

\item{version}{Whether to use R or C++ code to perform the
coordinate ascent updates.}

\item{e}{A small number to add to the diagonal elements of the
prior matrices to improve numerical stability of the updates.}

\item{ca_update_order}{The order with which coordinated are updated.
So far, "consecutive", "decreasing_logBF", "increasing_logBF" are supported.}
}
\value{
A mr.mash fit, stored as a list with some or all of the
following elements:

\item{mu1}{p x r matrix of posterior means for the regression
  coeffcients.}

\item{S1}{r x r x p array of posterior covariances for the
  regression coeffcients.}

\item{w1}{p x K matrix of posterior assignment probabilities to the
  mixture components.}
  
\item{V}{r x r residual covariance matrix}

\item{w0}{K-vector with (updated, if \code{update_w0=TRUE}) prior mixture weights, each associated with
  the respective covariance matrix in \code{S0}}.
  
\item{S0}{r x r x K array of prior covariance matrices
  on the regression coefficients}.

\item{intercept}{r-vector containing posterior mean estimate of the
  intercept.}

\item{fitted}{n x r matrix of fitted values.}

\item{ELBO}{Evidence Lower Bound (ELBO) at last iteration.}

\item{progress}{A data frame including information regarding
  convergence criteria at each iteration.}
  
\item{converged}{\code{TRUE} or \code{FALSE}, indicating whether
  the optimization algorithm converged to a solution within the chosen tolerance
  level.}
}
\description{
Performs multivariate multiple regression with
  mixture-of-normals prior.
}
\examples{
###Set seed
set.seed(123)

###Simulate X and Y
##Set parameters
n  <- 100
p <- 10

##Compute residual covariance
V  <- rbind(c(1.0,0.2),
            c(0.2,0.4))

##Set true effects
B  <- matrix(c(-2, -2,
               5, 5,
               rep(0, (p-2)*2)), byrow=TRUE, ncol=2)

##Simulate X
X <- matrix(rnorm(n*p), nrow=n, ncol=p)
X <- scale(X, center=TRUE, scale=FALSE)

##Simulate Y from MN(XB, I_n, V) where I_n is an nxn identity matrix
##and V is the residual covariance  
Y <- mr.mash.alpha:::sim_mvr(X, B, V)

###Specify the mixture weights and covariance matrices for the
### mixture-of-normals prior.
grid <- seq(1, 5)
S0mix <- mr.mash.alpha:::compute_cov_canonical(ncol(Y), singletons=TRUE,
           hetgrid=c(0, 0.25, 0.5, 0.75, 0.99), grid, zeromat=TRUE)
w0    <- rep(1/(length(S0mix)), length(S0mix))

###Split the data in training and test sets
Ytrain <- Y[-c(1:10), ]
Xtrain <- X[-c(1:10), ]
Ytest <- Y[c(1:10), ]
Xtest <- X[c(1:10), ]

###Estimate residual covariance
V_est <- cov(Ytrain)

###Fit mr.mash
fit <- mr.mash(Xtrain, Ytrain, S0mix, w0, V_est, tol=1e-8, update_w0=TRUE,
               update_w0_method="EM", compute_ELBO=TRUE, standardize=TRUE,
               verbose=TRUE, update_V=TRUE, version="R", e=1e-8)

# Compare the "fitted" values of Y against the true Y in the training set.
plot(fit$fitted,Ytrain,pch = 20,col = "darkblue",xlab = "true",
     ylab = "fitted")
abline(a = 0,b = 1,col = "magenta",lty = "dotted")

# Predict the multivariate outcomes in the test set using the fitted model.
Ytest_est <- predict(fit,Xtest)
plot(Ytest_est,Ytest,pch = 20,col = "darkblue",xlab = "true",
     ylab = "predicted")
abline(a = 0,b = 1,col = "magenta",lty = "dotted")

}
